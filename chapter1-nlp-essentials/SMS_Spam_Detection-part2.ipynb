{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SMS_Spam_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RS6j_Uu1aKr7",
        "Q5NI7lL_aPrR",
        "50W5rWfeOoR3"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "anlp37",
      "language": "python",
      "name": "anlp37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-w-m/anlp-tf2/blob/master/chapter1-nlp-essentials/SMS_Spam_Detection-part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5cgqQ3GJnXz"
      },
      "source": [
        "## Note that this notebook should be uploaded to Google Colab and run there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Zy9eSdb1cn4Z",
        "outputId": "dd3696fc-f0bf-4a81-ac1e-57e00a99aefc"
      },
      "source": [
        "#%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "#from tf.keras.models import Sequential\n",
        "#from tf.keras.layers import Dense\n",
        "import os\n",
        "import io\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ3vRSqLvTu6"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JSjikP_c0Ba",
        "outputId": "2ed149bf-2ee4-4e81-8772-0295f940869c"
      },
      "source": [
        "# Download the zip file\n",
        "path_to_zip = tf.keras.utils.get_file(\"smsspamcollection.zip\",\n",
        "                  origin=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\",\n",
        "                  extract=True)\n",
        "\n",
        "# Unzip the file into a folder\n",
        "!unzip $path_to_zip -d data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
            "204800/203415 [==============================] - 0s 1us/step\n",
            "Archive:  /root/.keras/datasets/smsspamcollection.zip\n",
            "  inflating: data/SMSSpamCollection  \n",
            "  inflating: data/readme             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytFnxCGFaJ20"
      },
      "source": [
        "# optional step - helps if colab gets disconnected\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb8AF3wHaKE3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3e8262d5-884d-4921-afca-4dd733bca421"
      },
      "source": [
        "# Test data reading\n",
        "#lines = io.open('/content/drive/My Drive/colab-data/SMSSpamCollection').read().strip().split('\\n')\n",
        "#lines = io.open('/content/data/SMSSpamCollection').read().strip().split('\\n')\n",
        "lines = io.open('data/SMSSpamCollection').read().strip().split('\\n')\n",
        "lines[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCFBMGdWvnNn"
      },
      "source": [
        "# Pre-Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhpfi9lWC5We",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04c92fdd-f388-41a1-c728-41ae6edc5c32"
      },
      "source": [
        "spam_dataset = []\n",
        "count = 0\n",
        "for line in lines:\n",
        "  label, text = line.split('\\t')\n",
        "  if label.lower().strip() == 'spam':\n",
        "    spam_dataset.append((1, text.strip()))\n",
        "    count += 1\n",
        "  else:\n",
        "    spam_dataset.append(((0, text.strip())))\n",
        "\n",
        "print(spam_dataset[0])\n",
        "print(\"Spam: \", count)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...')\n",
            "Spam:  747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SkoqwjizbBS"
      },
      "source": [
        "# Data Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsuhM6AI1iYM"
      },
      "source": [
        "import pandas as pd "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFCghF5YLcmb"
      },
      "source": [
        "df = pd.DataFrame(spam_dataset, columns=['Spam', 'Message'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRQZpvvHhI9K"
      },
      "source": [
        "import re\n",
        "\n",
        "# Normalization functions\n",
        "\n",
        "def message_length(x):\n",
        "  # returns total number of characters\n",
        "  return len(x)\n",
        "\n",
        "def num_capitals(x):\n",
        "  _, count = re.subn(r'[A-Z]', '', x) # only works in english\n",
        "  return count\n",
        "\n",
        "def num_punctuation(x):\n",
        "  _, count = re.subn(r'\\W', '', x)\n",
        "  return count\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaPsdhs6mkd_"
      },
      "source": [
        "df['Capitals'] = df['Message'].apply(num_capitals)\n",
        "df['Punctuation'] = df['Message'].apply(num_punctuation)\n",
        "df['Length'] = df['Message'].apply(message_length)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Oy5mN8nnrde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "bad6b5ea-4fe0-48b9-dfb2-5fa37f3bae94"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5574.000000</td>\n",
              "      <td>5574.000000</td>\n",
              "      <td>5574.000000</td>\n",
              "      <td>5574.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.134015</td>\n",
              "      <td>5.621636</td>\n",
              "      <td>18.942591</td>\n",
              "      <td>80.443488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.340699</td>\n",
              "      <td>11.683233</td>\n",
              "      <td>14.825994</td>\n",
              "      <td>59.841746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>36.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>61.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>122.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>910.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Spam     Capitals  Punctuation       Length\n",
              "count  5574.000000  5574.000000  5574.000000  5574.000000\n",
              "mean      0.134015     5.621636    18.942591    80.443488\n",
              "std       0.340699    11.683233    14.825994    59.841746\n",
              "min       0.000000     0.000000     0.000000     2.000000\n",
              "25%       0.000000     1.000000     8.000000    36.000000\n",
              "50%       0.000000     2.000000    15.000000    61.000000\n",
              "75%       0.000000     4.000000    27.000000   122.000000\n",
              "max       1.000000   129.000000   253.000000   910.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX7Ne4NLoPIE"
      },
      "source": [
        "train=df.sample(frac=0.8,random_state=42) #random state is a seed value\n",
        "test=df.drop(train.index)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FemsnVTto-c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "0b9a6344-05b5-44aa-8709-ba3d5dff7abf"
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.132765</td>\n",
              "      <td>5.519399</td>\n",
              "      <td>18.886522</td>\n",
              "      <td>80.316439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.339359</td>\n",
              "      <td>11.405424</td>\n",
              "      <td>14.602023</td>\n",
              "      <td>59.346407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>35.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>61.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>122.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>910.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Spam     Capitals  Punctuation       Length\n",
              "count  4459.000000  4459.000000  4459.000000  4459.000000\n",
              "mean      0.132765     5.519399    18.886522    80.316439\n",
              "std       0.339359    11.405424    14.602023    59.346407\n",
              "min       0.000000     0.000000     0.000000     2.000000\n",
              "25%       0.000000     1.000000     8.000000    35.000000\n",
              "50%       0.000000     2.000000    15.000000    61.000000\n",
              "75%       0.000000     4.000000    27.000000   122.000000\n",
              "max       1.000000   129.000000   253.000000   910.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEe1TMcApCYS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "7e3a2bee-6f7c-427c-aa2f-4a4fe9d0ec09"
      },
      "source": [
        "test.describe()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.139013</td>\n",
              "      <td>6.030493</td>\n",
              "      <td>19.166816</td>\n",
              "      <td>80.951570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.346116</td>\n",
              "      <td>12.731059</td>\n",
              "      <td>15.694599</td>\n",
              "      <td>61.807655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>36.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>61.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>123.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>127.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>790.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Spam     Capitals  Punctuation       Length\n",
              "count  1115.000000  1115.000000  1115.000000  1115.000000\n",
              "mean      0.139013     6.030493    19.166816    80.951570\n",
              "std       0.346116    12.731059    15.694599    61.807655\n",
              "min       0.000000     0.000000     0.000000     2.000000\n",
              "25%       0.000000     1.000000     8.000000    36.000000\n",
              "50%       0.000000     2.000000    15.000000    61.000000\n",
              "75%       0.000000     4.000000    28.000000   123.000000\n",
              "max       1.000000   127.000000   195.000000   790.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9rSWD4i1X3s"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WU6sxf2qcZd"
      },
      "source": [
        "# Basic 1-layer neural network model for evaluation\n",
        "def make_model(input_dims=3, num_units=12):\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # Adds a densely-connected layer with 12 units to the model:\n",
        "  model.add(tf.keras.layers.Dense(num_units, \n",
        "                                  input_dim=input_dims, \n",
        "                                  activation='relu'))\n",
        "\n",
        "  # Add a sigmoid layer with a binary output unit:\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', \n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwDyHEq-swwC"
      },
      "source": [
        "x_train = train[['Length', 'Punctuation', 'Capitals']]\n",
        "y_train = train[['Spam']]\n",
        "\n",
        "x_test = test[['Length', 'Punctuation', 'Capitals']]\n",
        "y_test = test[['Spam']]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEcbnfjYtxvu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "458f55a8-3d39-48a0-e739-bdce8c30627b"
      },
      "source": [
        "x_train"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Length</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Capitals</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3690</th>\n",
              "      <td>25</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3527</th>\n",
              "      <td>161</td>\n",
              "      <td>48</td>\n",
              "      <td>107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>724</th>\n",
              "      <td>40</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3370</th>\n",
              "      <td>69</td>\n",
              "      <td>17</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>468</th>\n",
              "      <td>37</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3280</th>\n",
              "      <td>444</td>\n",
              "      <td>114</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3186</th>\n",
              "      <td>65</td>\n",
              "      <td>14</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3953</th>\n",
              "      <td>81</td>\n",
              "      <td>23</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>38</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4223</th>\n",
              "      <td>65</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4459 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Length  Punctuation  Capitals\n",
              "3690      25            4         1\n",
              "3527     161           48       107\n",
              "724       40            7         1\n",
              "3370      69           17         3\n",
              "468       37            8         1\n",
              "...      ...          ...       ...\n",
              "3280     444          114        44\n",
              "3186      65           14        50\n",
              "3953      81           23         2\n",
              "2768      38            8         2\n",
              "4223      65           14         2\n",
              "\n",
              "[4459 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz1aTS9LuFpF"
      },
      "source": [
        "model = make_model()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otOL712wu4tW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc4f1a67-a487-4d52-aad6-bc9d8f3dd073"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "446/446 [==============================] - 4s 2ms/step - loss: 2.1590 - accuracy: 0.6615\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2808 - accuracy: 0.9261\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2249 - accuracy: 0.9342\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2142 - accuracy: 0.9398\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2055 - accuracy: 0.9374\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2098 - accuracy: 0.9339\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2065 - accuracy: 0.9345\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.1901 - accuracy: 0.9410\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.1866 - accuracy: 0.9391\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.1857 - accuracy: 0.9364\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f48abb1d510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svFEbDWzccXV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd80ced-ca62-4544-ecaf-782a239e6dc3"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2781 - accuracy: 0.8843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.27805662155151367, 0.884304940700531]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Aw3qskjDC-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d85e0d95-9c1e-4d9f-afc7-7f48167e63c4"
      },
      "source": [
        "y_train_pred = model.predict_classes(x_train)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c73dfRXaFD3F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf29c5b-4190-4662-9789-da6a0379fa92"
      },
      "source": [
        "# confusion matrix\n",
        "tf.math.confusion_matrix(tf.constant(y_train.Spam), \n",
        "                         y_train_pred)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
              "array([[3837,   30],\n",
              "       [ 457,  135]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmcZKcJqGiAK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49235136-0239-45f2-f259-26d9dae7386a"
      },
      "source": [
        "sum(y_train_pred)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([165], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUBeCm5eDduc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4954daa-c560-42e5-faa0-60212d8ae661"
      },
      "source": [
        "y_test_pred = model.predict_classes(x_test)\n",
        "tf.math.confusion_matrix(tf.constant(y_test.Spam), y_test_pred)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
              "array([[956,   4],\n",
              "       [125,  30]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY-ssfsAOusL"
      },
      "source": [
        "# Tokenization and Stop Word Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MWyaX6IPk2l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61cad08a-34bf-48cd-a208-2c79af7140b5"
      },
      "source": [
        "sentence = 'Go until jurong point, crazy.. Available only in bugis n great world'\n",
        "sentence.split()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Go',\n",
              " 'until',\n",
              " 'jurong',\n",
              " 'point,',\n",
              " 'crazy..',\n",
              " 'Available',\n",
              " 'only',\n",
              " 'in',\n",
              " 'bugis',\n",
              " 'n',\n",
              " 'great',\n",
              " 'world']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZvEl3bsRDrk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac540422-df34-43d4-89eb-643a3ab7637c"
      },
      "source": [
        "!pip install stanza  # StanfordNLP has become https://github.com/stanfordnlp/stanza/"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stanza\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/ae/a70a58ce6b4e2daad538688806ee0f238dbe601954582a74ea57cde6c532/stanza-1.2-py3-none-any.whl (282kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 23.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 16.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30kB 13.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 51kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 61kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 71kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 81kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 92kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 102kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 112kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 122kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 133kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 143kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 153kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 163kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 174kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 184kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 194kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 204kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 215kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 225kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 235kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 245kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 256kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 266kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 276kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.8.1+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (54.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n",
            "Installing collected packages: stanza\n",
            "Successfully installed stanza-1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRWnB-mNdH66"
      },
      "source": [
        "import stanza"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6N5vvi8dZS1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f97d9ebe-8e23-40bb-9023-9933e7dc691c"
      },
      "source": [
        "en = stanza.download('en') "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 28.1MB/s]                    \n",
            "2021-04-05 02:36:21 INFO: Downloading default packages for language: en (English)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.2.0/en/default.zip: 100%|██████████| 411M/411M [01:13<00:00, 5.57MB/s]\n",
            "2021-04-05 02:37:40 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxDrEJBBdfkq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "687a6a15-61a5-4d9a-f284-37634d1c3c5a"
      },
      "source": [
        "en = stanza.Pipeline(lang='en')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-05 02:37:40 INFO: Loading these models for language: en (English):\n",
            "=========================\n",
            "| Processor | Package   |\n",
            "-------------------------\n",
            "| tokenize  | combined  |\n",
            "| pos       | combined  |\n",
            "| lemma     | combined  |\n",
            "| depparse  | combined  |\n",
            "| sentiment | sstplus   |\n",
            "| ner       | ontonotes |\n",
            "=========================\n",
            "\n",
            "2021-04-05 02:37:40 INFO: Use device: gpu\n",
            "2021-04-05 02:37:40 INFO: Loading: tokenize\n",
            "2021-04-05 02:37:51 INFO: Loading: pos\n",
            "2021-04-05 02:37:51 INFO: Loading: lemma\n",
            "2021-04-05 02:37:51 INFO: Loading: depparse\n",
            "2021-04-05 02:37:52 INFO: Loading: sentiment\n",
            "2021-04-05 02:37:52 INFO: Loading: ner\n",
            "2021-04-05 02:37:53 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FlNpKzX2ggg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "759cab0f-8d90-4862-dce1-3d7ae0c69033"
      },
      "source": [
        "sentence"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Go until jurong point, crazy.. Available only in bugis n great world'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr4UixONeSAd"
      },
      "source": [
        "tokenized = en(sentence)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPq8hvA3he4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66417cde-3ed1-46d6-8a8e-9dca98a8ed40"
      },
      "source": [
        "len(tokenized.sentences)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJarHilWhreq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a57516-fe10-4029-ef84-91579ef10ac9"
      },
      "source": [
        "for snt in tokenized.sentences:\n",
        "  for word in snt.tokens:\n",
        "    print(word.text)\n",
        "  print(\"<End of Sentence>\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go\n",
            "until\n",
            "jurong\n",
            "point\n",
            ",\n",
            "crazy\n",
            "..\n",
            "<End of Sentence>\n",
            "Available\n",
            "only\n",
            "in\n",
            "bugis\n",
            "n\n",
            "great\n",
            "world\n",
            "<End of Sentence>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS6j_Uu1aKr7"
      },
      "source": [
        "## Dependency Parsing Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRlPCZxdffIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da6b4473-8065-4799-b804-e7f5b33ae9f2"
      },
      "source": [
        "en2 = stanza.Pipeline(lang='en')\n",
        "pr2 = en2(\"Hari went to school\")\n",
        "for snt in pr2.sentences:\n",
        "  for word in snt.tokens:\n",
        "    print(word)\n",
        "  print(\"<End of Sentence>\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-05 02:37:53 INFO: Loading these models for language: en (English):\n",
            "=========================\n",
            "| Processor | Package   |\n",
            "-------------------------\n",
            "| tokenize  | combined  |\n",
            "| pos       | combined  |\n",
            "| lemma     | combined  |\n",
            "| depparse  | combined  |\n",
            "| sentiment | sstplus   |\n",
            "| ner       | ontonotes |\n",
            "=========================\n",
            "\n",
            "2021-04-05 02:37:53 INFO: Use device: gpu\n",
            "2021-04-05 02:37:53 INFO: Loading: tokenize\n",
            "2021-04-05 02:37:53 INFO: Loading: pos\n",
            "2021-04-05 02:37:54 INFO: Loading: lemma\n",
            "2021-04-05 02:37:54 INFO: Loading: depparse\n",
            "2021-04-05 02:37:54 INFO: Loading: sentiment\n",
            "2021-04-05 02:37:55 INFO: Loading: ner\n",
            "2021-04-05 02:37:55 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    \"id\": 1,\n",
            "    \"text\": \"Hari\",\n",
            "    \"lemma\": \"Hari\",\n",
            "    \"upos\": \"PROPN\",\n",
            "    \"xpos\": \"NNP\",\n",
            "    \"feats\": \"Number=Sing\",\n",
            "    \"head\": 2,\n",
            "    \"deprel\": \"nsubj\",\n",
            "    \"misc\": \"start_char=0|end_char=4\",\n",
            "    \"ner\": \"S-PERSON\"\n",
            "  }\n",
            "]\n",
            "[\n",
            "  {\n",
            "    \"id\": 2,\n",
            "    \"text\": \"went\",\n",
            "    \"lemma\": \"go\",\n",
            "    \"upos\": \"VERB\",\n",
            "    \"xpos\": \"VBD\",\n",
            "    \"feats\": \"Mood=Ind|Tense=Past|VerbForm=Fin\",\n",
            "    \"head\": 0,\n",
            "    \"deprel\": \"root\",\n",
            "    \"misc\": \"start_char=5|end_char=9\",\n",
            "    \"ner\": \"O\"\n",
            "  }\n",
            "]\n",
            "[\n",
            "  {\n",
            "    \"id\": 3,\n",
            "    \"text\": \"to\",\n",
            "    \"lemma\": \"to\",\n",
            "    \"upos\": \"ADP\",\n",
            "    \"xpos\": \"IN\",\n",
            "    \"head\": 4,\n",
            "    \"deprel\": \"case\",\n",
            "    \"misc\": \"start_char=10|end_char=12\",\n",
            "    \"ner\": \"O\"\n",
            "  }\n",
            "]\n",
            "[\n",
            "  {\n",
            "    \"id\": 4,\n",
            "    \"text\": \"school\",\n",
            "    \"lemma\": \"school\",\n",
            "    \"upos\": \"NOUN\",\n",
            "    \"xpos\": \"NN\",\n",
            "    \"feats\": \"Number=Sing\",\n",
            "    \"head\": 2,\n",
            "    \"deprel\": \"obl\",\n",
            "    \"misc\": \"start_char=13|end_char=19\",\n",
            "    \"ner\": \"O\"\n",
            "  }\n",
            "]\n",
            "<End of Sentence>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5NI7lL_aPrR"
      },
      "source": [
        "## Japanese Tokenization Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twQsK3NSRh3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "d0aa1977-9a22-4251-e1a8-7a90f8b483cb"
      },
      "source": [
        "jp = stanza.download('ja') "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 36.2MB/s]                    \n",
            "2021-04-05 02:37:55 INFO: Downloading default packages for language: ja (Japanese)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.2.0/ja/default.zip:  60%|██████    | 124M/205M [00:18<00:12, 6.61MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-5c11f269c0b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ja'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stanza/resources/common.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(lang, model_dir, package, processors, logging_level, verbose, resources_url, resources_branch, resources_version, model_url)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;34mf'{url}/{resources_version}/{lang}/default.zip'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'default.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'default_md5'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         )\n\u001b[1;32m    413\u001b[0m         \u001b[0munzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stanza/resources/common.py\u001b[0m in \u001b[0;36mrequest_file\u001b[0;34m(url, path, md5)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'File exists: {path}.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mmd5\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stanza/resources/common.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(url, path)\u001b[0m\n\u001b[1;32m    116\u001b[0m         with tqdm(total=file_size, unit='B', unit_scale=True, \\\n\u001b[1;32m    117\u001b[0m             disable=not verbose, desc=desc) as pbar:\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_chunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0;31m# Close the connection when no data is returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRHQL57iRnH2"
      },
      "source": [
        "jp = stanza.Pipeline(lang='ja')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM_AJ4mCS2ja"
      },
      "source": [
        "jp_line = jp(\"選挙管理委員会\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMo9DqvMTzX_"
      },
      "source": [
        "for snt in jp_line.sentences:\n",
        "  for word in snt.tokens:\n",
        "    print(word.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nKfjQeOaWtz"
      },
      "source": [
        "# Adding Word Count Feature "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBK3MokpYa6n"
      },
      "source": [
        "def word_counts(x, pipeline=en):\n",
        "  doc = pipeline(x)\n",
        "  count = sum( [ len(sentence.tokens) for sentence in doc.sentences] )\n",
        "  return count\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQf6MQNmbazh"
      },
      "source": [
        "#en = snlp.Pipeline(lang='en', processors='tokenize')\n",
        "df['Words'] = df['Message'].apply(word_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLjJV6gAc7Sh"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-pqZ1a3dOg2"
      },
      "source": [
        "#train=df.sample(frac=0.8,random_state=42) #random state is a seed value\n",
        "#test=df.drop(train.index)\n",
        "\n",
        "train['Words'] = train['Message'].apply(word_counts)\n",
        "test['Words'] = test['Message'].apply(word_counts)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lruns2ddQzT"
      },
      "source": [
        "x_train = train[['Length', 'Punctuation', 'Capitals', 'Words']]\n",
        "y_train = train[['Spam']]\n",
        "\n",
        "x_test = test[['Length', 'Punctuation', 'Capitals' , 'Words']]\n",
        "y_test = test[['Spam']]\n",
        "\n",
        "model = make_model(input_dims=4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8So4_u34daa5"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw__r38sdv60"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH43JjrwS9Ta"
      },
      "source": [
        "## Stop Word Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukeJuqpdmKI2"
      },
      "source": [
        "!pip install stopwordsiso"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "E-kxi8XBTA1r",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "source": [
        "import stopwordsiso as stopwords\n",
        "\n",
        "stopwords.langs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "RE093gWDT57H",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "source": [
        "sorted(stopwords.stopwords('en'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXUXOkhgnRol"
      },
      "source": [
        "en_sw = stopwords.stopwords('en')\n",
        "\n",
        "def word_counts(x, pipeline=en):\n",
        "  doc = pipeline(x)\n",
        "  count = 0\n",
        "  for sentence in doc.sentences:\n",
        "    for token in sentence.tokens:\n",
        "        if token.text.lower() not in en_sw:\n",
        "          count += 1\n",
        "  return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trbxkc1B4sNP"
      },
      "source": [
        "train['Words'] = train['Message'].apply(word_counts)\n",
        "test['Words'] = test['Message'].apply(word_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXxl8S1vCGi3",
        "tags": []
      },
      "source": [
        "x_train = train[['Length', 'Punctuation', 'Capitals', 'Words']]\n",
        "y_train = train[['Spam']]\n",
        "\n",
        "x_test = test[['Length', 'Punctuation', 'Capitals' , 'Words']]\n",
        "y_test = test[['Spam']]\n",
        "\n",
        "model = make_model(input_dims=4)\n",
        "#model = make_model(input_dims=3)\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4-0huTw5YFe"
      },
      "source": [
        "## POS Based Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bSXLgr6iiB0"
      },
      "source": [
        "en = stanza.Pipeline(lang='en')\n",
        "\n",
        "txt = \"Yo you around? A friend of mine's lookin.\"\n",
        "pos = en(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD9BUu6gkNi_"
      },
      "source": [
        "def print_pos(doc):\n",
        "    text = \"\"\n",
        "    for sentence in doc.sentences:\n",
        "        for token in sentence.tokens:\n",
        "            text += token.words[0].text + \"/\" + \\\n",
        "                    token.words[0].upos + \" \"\n",
        "        text += \"\\n\"\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTLrN-BgkuWV"
      },
      "source": [
        "print(print_pos(pos))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i_BcxgK5cs6"
      },
      "source": [
        "en_sw = stopwords.stopwords('en')\n",
        "\n",
        "def word_counts_v3(x, pipeline=en):\n",
        "  doc = pipeline(x)\n",
        "  count = 0\n",
        "  for sentence in doc.sentences:\n",
        "    for token in sentence.tokens:\n",
        "        if token.text.lower() not in en_sw and \\\n",
        "        token.words[0].upos not in ['PUNCT', 'SYM']:\n",
        "          count += 1\n",
        "  return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4ddhhiY9FgG"
      },
      "source": [
        "print(word_counts(txt), word_counts_v3(txt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99upx22tCHgz"
      },
      "source": [
        "train['Test'] = 0\n",
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGkdUgEdEDcB"
      },
      "source": [
        "def word_counts_v3(x, pipeline=en):\n",
        "  doc = pipeline(x)\n",
        "  totals = 0.\n",
        "  count = 0.\n",
        "  non_word = 0.\n",
        "  for sentence in doc.sentences:\n",
        "    totals += len(sentence.tokens)  # (1)\n",
        "    for token in sentence.tokens:\n",
        "        if token.text.lower() not in en_sw:\n",
        "          if token.words[0].upos not in ['PUNCT', 'SYM']:\n",
        "            count += 1.\n",
        "          else:\n",
        "            non_word += 1.\n",
        "  non_word = non_word / totals\n",
        "  return pd.Series([count, non_word], index=['Words_NoPunct', 'Punct'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6LWE7QWIuyu"
      },
      "source": [
        "x = train[:10]\n",
        "x.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97Y6_7E8KJnQ"
      },
      "source": [
        "train_tmp = train['Message'].apply(word_counts_v3)\n",
        "train = pd.concat([train, train_tmp], axis=1)\n",
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WUdTWXvWbvc"
      },
      "source": [
        "test_tmp = test['Message'].apply(word_counts_v3)\n",
        "test = pd.concat([test, test_tmp], axis=1)\n",
        "test.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMv1q8_mKdjI"
      },
      "source": [
        "z = pd.concat([x, train_tmp], axis=1)\n",
        "z.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jkFZUJcPHA3"
      },
      "source": [
        "z.loc[z['Spam']==0].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l86GIgYvP9Mc"
      },
      "source": [
        "z.loc[z['Spam']==1].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-PdmMu_R01u"
      },
      "source": [
        "aa = [word_counts_v3(y) for y in x['Message']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBxbhHD_SMPH"
      },
      "source": [
        "ab = pd.DataFrame(aa)\n",
        "ab.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__TVLnoy-nre"
      },
      "source": [
        "# Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbXe_2yL-qb4"
      },
      "source": [
        "\n",
        "text = \"Stemming is aimed at reducing vocabulary and aid un-derstanding of\" +\\\n",
        "       \" morphological processes. This helps people un-derstand the\" +\\\n",
        "       \" morphology of words and reduce size of corpus.\"\n",
        "\n",
        "lemma = en(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8VfJ07pDEgN"
      },
      "source": [
        "lemmas = \"\"\n",
        "for sentence in lemma.sentences:\n",
        "        for token in sentence.tokens:\n",
        "            lemmas += token.words[0].lemma +\"/\" + \\\n",
        "                    token.words[0].upos + \" \"\n",
        "        lemmas += \"\\n\"\n",
        "\n",
        "print(lemmas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50W5rWfeOoR3"
      },
      "source": [
        "# TF-IDF Based Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy7RPjvfDRXz"
      },
      "source": [
        "# if not installed already\n",
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cco-INbyhouu"
      },
      "source": [
        "corpus = [\n",
        "          \"I like fruits. Fruits like bananas\",\n",
        "          \"I love bananas but eat an apple\",\n",
        "          \"An apple a day keeps the doctor away\"\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbftF_xOEN9O"
      },
      "source": [
        "## Count Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clVOFDb1ERTL"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM7jlBZdFmiD"
      },
      "source": [
        "X.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pbT9sH-GwvM"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "cosine_similarity(X.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe6qPCQ4Gxb0"
      },
      "source": [
        "query = vectorizer.transform([\"apple and bananas\"])\n",
        "\n",
        "cosine_similarity(X, query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w2zVnOnER_3"
      },
      "source": [
        "## TF-IDF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKyNZgATHmhM"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "transformer = TfidfTransformer(smooth_idf=False)\n",
        "tfidf = transformer.fit_transform(X.toarray())\n",
        "\n",
        "pd.DataFrame(tfidf.toarray(), \n",
        "             columns=vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rifoynAZvO9H"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "tfidf = TfidfVectorizer(binary=True)\n",
        "X = tfidf.fit_transform(train['Message']).astype('float32')\n",
        "X_test = tfidf.transform(test['Message']).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDnG2OPHwD7U"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGujuysLwKJe"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "_, cols = X.shape\n",
        "model2 = make_model(cols)  # to match tf-idf dimensions\n",
        "lb = LabelEncoder()\n",
        "y = lb.fit_transform(y_train)\n",
        "dummy_y_train = np_utils.to_categorical(y)\n",
        "model2.fit(X.toarray(), y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXYe6f8my21Y"
      },
      "source": [
        "model2.evaluate(X_test.toarray(), y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptf40YDjh71c",
        "tags": []
      },
      "source": [
        "train.loc[train.Spam == 1].describe() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMMB-9H3IEwm"
      },
      "source": [
        "# Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "nXyY5EUkIGAS",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "source": [
        "# memory limit may be exceeded. Try deleting some objects before running this next section\n",
        "# or copy this section to a different notebook.\n",
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OtFF5X_IZJ4"
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "import gensim.downloader as api"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "6hNrv_O2nPgD",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "source": [
        "api.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2nh6Th24Jnq"
      },
      "source": [
        "model_w2v = api.load\"word2vec-google-news-300\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WZabbSCjbeT"
      },
      "source": [
        "__deprecated__\n",
        "```python\n",
        "model_w2v = api.load\"word2vec-google-news-300\")\n",
        "```\n",
        "__runtime error__\n",
        "```shell\n",
        "[=========-----------------------------------------] 18.0% 299.5/1662.8MB downloaded\n",
        "---------------------------------------------------------------------------\n",
        "ConnectionResetError                      Traceback (most recent call last)\n",
        "<ipython-input-135-1bce0bfee7a7> in <module>()\n",
        "----> 1 model_w2v = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "7 frames\n",
        "/usr/lib/python3.7/ssl.py in read(self, len, buffer)\n",
        "    927         try:\n",
        "    928             if buffer is not None:\n",
        "--> 929                 return self._sslobj.read(len, buffer)\n",
        "    930             else:\n",
        "    931                 return self._sslobj.read(len)\n",
        "\n",
        "ConnectionResetError: [Errno 104] Connection reset by peer\n",
        "```\n",
        "__local run__\n",
        "```shell\n",
        "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: \n",
        "\n",
        "UserWarning: This function is deprecated, use smart_open.open instead. \n",
        "\n",
        "See the migration notes for details: \n",
        "https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
        "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
        "```\n",
        "\n",
        "* [RaRe-Technologies/smart_open/README.rst](https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function)\n",
        "\n",
        "* [Migration Guide](https://github.com/RaRe-Technologies/smart_open/blob/master/MIGRATING_FROM_OLDER_VERSIONS.rst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53VpEeANkx0l"
      },
      "source": [
        "__this did NOT work__\n",
        "* [bhaettasch/gensim_word2vec_demo.py](https://gist.github.com/bhaettasch/d7f4e22e79df3c8b6c20)\n",
        "\n",
        "__growupboron__ comment on Aug 24, 2020\n",
        "Since this example this deprecated, I created a [Google Colab demo](https://colab.research.google.com/drive/1aLNhDu1qtQnNIvN5Hhl0UhNBBe84N9S4?usp=sharing) for the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OSTOe4ikPdv"
      },
      "source": [
        "\"\"\"\n",
        "# download and extract the Google News Dataset\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://drive.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://drive.google.com/uc?export=download&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\" -O GoogleNews-vectors-negative300.bin.gz && rm -rf /tmp/cookies.txt\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h9O7A9olWzB"
      },
      "source": [
        "\"\"\"\n",
        "import gensim\n",
        "from gensim import models\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
        "#model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, norm_only=True) -> Deprecated\n",
        "model_w2v = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) #without *norm_only* param\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0lxhBMeIYrl",
        "tags": []
      },
      "source": [
        "model_w2v.most_similar(\"cookies\",topn=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rkfcI1niIYQ4",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "source": [
        "model_w2v.doesnt_match([\"USA\",\"Canada\",\"India\",\"Tokyo\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iZ0lCjnaImWZ",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "source": [
        "king = model_w2v['king']\n",
        "man = model_w2v['man']\n",
        "woman = model_w2v['woman']\n",
        "\n",
        "queen = king - man + woman  \n",
        "model_w2v.similar_by_vector(queen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2SSPqujo4K8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}