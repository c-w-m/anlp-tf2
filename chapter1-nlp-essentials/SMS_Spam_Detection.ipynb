{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SMS_Spam_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RS6j_Uu1aKr7",
        "Q5NI7lL_aPrR",
        "50W5rWfeOoR3"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "anlp37",
      "language": "python",
      "name": "anlp37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-w-m/anlp-tf2/blob/master/chapter1-nlp-essentials/SMS_Spam_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5cgqQ3GJnXz"
      },
      "source": [
        "## Note that this notebook should be uploaded to Google Colab and run there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Zy9eSdb1cn4Z",
        "outputId": "27451334-0994-4189-984a-ac6b9b98dbab"
      },
      "source": [
        "#%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "#from tf.keras.models import Sequential\n",
        "#from tf.keras.layers import Dense\n",
        "import os\n",
        "import io\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ3vRSqLvTu6"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JSjikP_c0Ba",
        "outputId": "fd3e60b5-c579-4c1f-a456-525daa5b7f29"
      },
      "source": [
        "# Download the zip file\n",
        "path_to_zip = tf.keras.utils.get_file(\"smsspamcollection.zip\",\n",
        "                  origin=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\",\n",
        "                  extract=True)\n",
        "\n",
        "# Unzip the file into a folder\n",
        "!unzip $path_to_zip -d data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /root/.keras/datasets/smsspamcollection.zip\n",
            "replace data/SMSSpamCollection? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytFnxCGFaJ20"
      },
      "source": [
        "# optional step - helps if colab gets disconnected\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb8AF3wHaKE3"
      },
      "source": [
        "# Test data reading\n",
        "lines = io.open('/content/drive/My Drive/colab-data/SMSSpamCollection').read().strip().split('\\n')\n",
        "#lines = io.open('/content/data/SMSSpamCollection').read().strip().split('\\n')\n",
        "#lines = io.open('data/SMSSpamCollection').read().strip().split('\\n')\n",
        "lines[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCFBMGdWvnNn"
      },
      "source": [
        "# Pre-Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhpfi9lWC5We"
      },
      "source": [
        "spam_dataset = []\n",
        "count = 0\n",
        "for line in lines:\n",
        "  label, text = line.split('\\t')\n",
        "  if label.lower().strip() == 'spam':\n",
        "    spam_dataset.append((1, text.strip()))\n",
        "    count += 1\n",
        "  else:\n",
        "    spam_dataset.append(((0, text.strip())))\n",
        "\n",
        "print(spam_dataset[0])\n",
        "print(\"Spam: \", count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SkoqwjizbBS"
      },
      "source": [
        "# Data Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsuhM6AI1iYM"
      },
      "source": [
        "import pandas as pd "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFCghF5YLcmb"
      },
      "source": [
        "df = pd.DataFrame(spam_dataset, columns=['Spam', 'Message'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRQZpvvHhI9K"
      },
      "source": [
        "import re\n",
        "\n",
        "# Normalization functions\n",
        "\n",
        "def message_length(x):\n",
        "  # returns total number of characters\n",
        "  return len(x)\n",
        "\n",
        "def num_capitals(x):\n",
        "  _, count = re.subn(r'[A-Z]', '', x) # only works in english\n",
        "  return count\n",
        "\n",
        "def num_punctuation(x):\n",
        "  _, count = re.subn(r'\\W', '', x)\n",
        "  return count\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaPsdhs6mkd_"
      },
      "source": [
        "df['Capitals'] = df['Message'].apply(num_capitals)\n",
        "df['Punctuation'] = df['Message'].apply(num_punctuation)\n",
        "df['Length'] = df['Message'].apply(message_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Oy5mN8nnrde"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX7Ne4NLoPIE"
      },
      "source": [
        "train=df.sample(frac=0.8,random_state=42) #random state is a seed value\n",
        "test=df.drop(train.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FemsnVTto-c6"
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEe1TMcApCYS"
      },
      "source": [
        "test.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9rSWD4i1X3s"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WU6sxf2qcZd"
      },
      "source": [
        "# Basic 1-layer neural network model for evaluation\n",
        "def make_model(input_dims=3, num_units=12):\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # Adds a densely-connected layer with 12 units to the model:\n",
        "  model.add(tf.keras.layers.Dense(num_units, \n",
        "                                  input_dim=input_dims, \n",
        "                                  activation='relu'))\n",
        "\n",
        "  # Add a sigmoid layer with a binary output unit:\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', \n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwDyHEq-swwC"
      },
      "source": [
        "x_train = train[['Length', 'Punctuation', 'Capitals']]\n",
        "y_train = train[['Spam']]\n",
        "\n",
        "x_test = test[['Length', 'Punctuation', 'Capitals']]\n",
        "y_test = test[['Spam']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEcbnfjYtxvu"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz1aTS9LuFpF"
      },
      "source": [
        "model = make_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otOL712wu4tW"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svFEbDWzccXV"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Aw3qskjDC-j"
      },
      "source": [
        "y_train_pred = model.predict_classes(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c73dfRXaFD3F"
      },
      "source": [
        "# confusion matrix\n",
        "tf.math.confusion_matrix(tf.constant(y_train.Spam), \n",
        "                         y_train_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmcZKcJqGiAK"
      },
      "source": [
        "sum(y_train_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUBeCm5eDduc"
      },
      "source": [
        "y_test_pred = model.predict_classes(x_test)\n",
        "tf.math.confusion_matrix(tf.constant(y_test.Spam), y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY-ssfsAOusL"
      },
      "source": [
        "# Tokenization and Stop Word Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MWyaX6IPk2l"
      },
      "source": [
        "sentence = 'Go until jurong point, crazy.. Available only in bugis n great world'\n",
        "sentence.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZvEl3bsRDrk"
      },
      "source": [
        "!pip install stanza  # StanfordNLP has become https://github.com/stanfordnlp/stanza/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRWnB-mNdH66"
      },
      "source": [
        "import stanza"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6N5vvi8dZS1"
      },
      "source": [
        "en = stanza.download('en') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxDrEJBBdfkq"
      },
      "source": [
        "en = stanza.Pipeline(lang='en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FlNpKzX2ggg"
      },
      "source": [
        "sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr4UixONeSAd"
      },
      "source": [
        "tokenized = en(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPq8hvA3he4C"
      },
      "source": [
        "len(tokenized.sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJarHilWhreq"
      },
      "source": [
        "for snt in tokenized.sentences:\n",
        "  for word in snt.tokens:\n",
        "    print(word.text)\n",
        "  print(\"<End of Sentence>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS6j_Uu1aKr7"
      },
      "source": [
        "## Dependency Parsing Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRlPCZxdffIi"
      },
      "source": [
        "en2 = stanza.Pipeline(lang='en')\n",
        "pr2 = en2(\"Hari went to school\")\n",
        "for snt in pr2.sentences:\n",
        "  for word in snt.tokens:\n",
        "    print(word)\n",
        "  print(\"<End of Sentence>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5NI7lL_aPrR"
      },
      "source": [
        "## Japanese Tokenization Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twQsK3NSRh3e"
      },
      "source": [
        "jp = stanza.download('ja') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRHQL57iRnH2"
      },
      "source": [
        "jp = stanza.Pipeline(lang='ja')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM_AJ4mCS2ja"
      },
      "source": [
        "jp_line = jp(\"選挙管理委員会\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMo9DqvMTzX_"
      },
      "source": [
        "for snt in jp_line.sentences:\n",
        "  for word in snt.tokens:\n",
        "    print(word.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nKfjQeOaWtz"
      },
      "source": [
        "# Adding Word Count Feature "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBK3MokpYa6n"
      },
      "source": [
        "def word_counts(x, pipeline=en):\n",
        "  doc = pipeline(x)\n",
        "  count = sum( [ len(sentence.tokens) for sentence in doc.sentences] )\n",
        "  return count\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQf6MQNmbazh"
      },
      "source": [
        "#en = snlp.Pipeline(lang='en', processors='tokenize')\n",
        "df['Words'] = df['Message'].apply(word_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLjJV6gAc7Sh"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-pqZ1a3dOg2"
      },
      "source": [
        "#train=df.sample(frac=0.8,random_state=42) #random state is a seed value\n",
        "#test=df.drop(train.index)\n",
        "\n",
        "train['Words'] = train['Message'].apply(word_counts)\n",
        "test['Words'] = test['Message'].apply(word_counts)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lruns2ddQzT"
      },
      "source": [
        "x_train = train[['Length', 'Punctuation', 'Capitals', 'Words']]\n",
        "y_train = train[['Spam']]\n",
        "\n",
        "x_test = test[['Length', 'Punctuation', 'Capitals' , 'Words']]\n",
        "y_test = test[['Spam']]\n",
        "\n",
        "model = make_model(input_dims=4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8So4_u34daa5"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw__r38sdv60"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH43JjrwS9Ta"
      },
      "source": [
        "## Stop Word Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukeJuqpdmKI2"
      },
      "source": [
        "!pip install stopwordsiso"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "E-kxi8XBTA1r",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "source": [
        "import stopwordsiso as stopwords\n",
        "\n",
        "stopwords.langs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "RE093gWDT57H",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "source": [
        "sorted(stopwords.stopwords('en'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXUXOkhgnRol"
      },
      "source": [
        "en_sw = stopwords.stopwords('en')\n",
        "\n",
        "def word_counts(x, pipeline=en):\n",
        "  doc = pipeline(x)\n",
        "  count = 0\n",
        "  for sentence in doc.sentences:\n",
        "    for token in sentence.tokens:\n",
        "        if token.text.lower() not in en_sw:\n",
        "          count += 1\n",
        "  return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trbxkc1B4sNP"
      },
      "source": [
        "train['Words'] = train['Message'].apply(word_counts)\n",
        "test['Words'] = test['Message'].apply(word_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXxl8S1vCGi3",
        "tags": []
      },
      "source": [
        "x_train = train[['Length', 'Punctuation', 'Capitals', 'Words']]\n",
        "y_train = train[['Spam']]\n",
        "\n",
        "x_test = test[['Length', 'Punctuation', 'Capitals' , 'Words']]\n",
        "y_test = test[['Spam']]\n",
        "\n",
        "model = make_model(input_dims=4)\n",
        "#model = make_model(input_dims=3)\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4-0huTw5YFe"
      },
      "source": [
        "## POS Based Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bSXLgr6iiB0"
      },
      "source": [
        "en = stanza.Pipeline(lang='en')\n",
        "\n",
        "txt = \"Yo you around? A friend of mine's lookin.\"\n",
        "pos = en(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD9BUu6gkNi_"
      },
      "source": [
        "def print_pos(doc):\n",
        "    text = \"\"\n",
        "    for sentence in doc.sentences:\n",
        "        for token in sentence.tokens:\n",
        "            text += token.words[0].text + \"/\" + \\\n",
        "                    token.words[0].upos + \" \"\n",
        "        text += \"\\n\"\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTLrN-BgkuWV"
      },
      "source": [
        "print(print_pos(pos))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i_BcxgK5cs6"
      },
      "source": [
        "en_sw = stopwords.stopwords('en')\n",
        "\n",
        "def word_counts_v3(x, pipeline=en):\n",
        "  doc = pipeline(x)\n",
        "  count = 0\n",
        "  for sentence in doc.sentences:\n",
        "    for token in sentence.tokens:\n",
        "        if token.text.lower() not in en_sw and \\\n",
        "        token.words[0].upos not in ['PUNCT', 'SYM']:\n",
        "          count += 1\n",
        "  return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4ddhhiY9FgG"
      },
      "source": [
        "print(word_counts(txt), word_counts_v3(txt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99upx22tCHgz"
      },
      "source": [
        "train['Test'] = 0\n",
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGkdUgEdEDcB"
      },
      "source": [
        "def word_counts_v3(x, pipeline=en):\n",
        "  doc = pipeline(x)\n",
        "  totals = 0.\n",
        "  count = 0.\n",
        "  non_word = 0.\n",
        "  for sentence in doc.sentences:\n",
        "    totals += len(sentence.tokens)  # (1)\n",
        "    for token in sentence.tokens:\n",
        "        if token.text.lower() not in en_sw:\n",
        "          if token.words[0].upos not in ['PUNCT', 'SYM']:\n",
        "            count += 1.\n",
        "          else:\n",
        "            non_word += 1.\n",
        "  non_word = non_word / totals\n",
        "  return pd.Series([count, non_word], index=['Words_NoPunct', 'Punct'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6LWE7QWIuyu"
      },
      "source": [
        "x = train[:10]\n",
        "x.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97Y6_7E8KJnQ"
      },
      "source": [
        "train_tmp = train['Message'].apply(word_counts_v3)\n",
        "train = pd.concat([train, train_tmp], axis=1)\n",
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WUdTWXvWbvc"
      },
      "source": [
        "test_tmp = test['Message'].apply(word_counts_v3)\n",
        "test = pd.concat([test, test_tmp], axis=1)\n",
        "test.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMv1q8_mKdjI"
      },
      "source": [
        "z = pd.concat([x, train_tmp], axis=1)\n",
        "z.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jkFZUJcPHA3"
      },
      "source": [
        "z.loc[z['Spam']==0].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l86GIgYvP9Mc"
      },
      "source": [
        "z.loc[z['Spam']==1].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-PdmMu_R01u"
      },
      "source": [
        "aa = [word_counts_v3(y) for y in x['Message']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBxbhHD_SMPH"
      },
      "source": [
        "ab = pd.DataFrame(aa)\n",
        "ab.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__TVLnoy-nre"
      },
      "source": [
        "# Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbXe_2yL-qb4"
      },
      "source": [
        "\n",
        "text = \"Stemming is aimed at reducing vocabulary and aid un-derstanding of\" +\\\n",
        "       \" morphological processes. This helps people un-derstand the\" +\\\n",
        "       \" morphology of words and reduce size of corpus.\"\n",
        "\n",
        "lemma = en(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8VfJ07pDEgN"
      },
      "source": [
        "lemmas = \"\"\n",
        "for sentence in lemma.sentences:\n",
        "        for token in sentence.tokens:\n",
        "            lemmas += token.words[0].lemma +\"/\" + \\\n",
        "                    token.words[0].upos + \" \"\n",
        "        lemmas += \"\\n\"\n",
        "\n",
        "print(lemmas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50W5rWfeOoR3"
      },
      "source": [
        "# TF-IDF Based Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy7RPjvfDRXz"
      },
      "source": [
        "# if not installed already\n",
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cco-INbyhouu"
      },
      "source": [
        "corpus = [\n",
        "          \"I like fruits. Fruits like bananas\",\n",
        "          \"I love bananas but eat an apple\",\n",
        "          \"An apple a day keeps the doctor away\"\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbftF_xOEN9O"
      },
      "source": [
        "## Count Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clVOFDb1ERTL"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM7jlBZdFmiD"
      },
      "source": [
        "X.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pbT9sH-GwvM"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "cosine_similarity(X.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe6qPCQ4Gxb0"
      },
      "source": [
        "query = vectorizer.transform([\"apple and bananas\"])\n",
        "\n",
        "cosine_similarity(X, query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w2zVnOnER_3"
      },
      "source": [
        "## TF-IDF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKyNZgATHmhM"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "transformer = TfidfTransformer(smooth_idf=False)\n",
        "tfidf = transformer.fit_transform(X.toarray())\n",
        "\n",
        "pd.DataFrame(tfidf.toarray(), \n",
        "             columns=vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rifoynAZvO9H"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "tfidf = TfidfVectorizer(binary=True)\n",
        "X = tfidf.fit_transform(train['Message']).astype('float32')\n",
        "X_test = tfidf.transform(test['Message']).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDnG2OPHwD7U"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGujuysLwKJe"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "_, cols = X.shape\n",
        "model2 = make_model(cols)  # to match tf-idf dimensions\n",
        "lb = LabelEncoder()\n",
        "y = lb.fit_transform(y_train)\n",
        "dummy_y_train = np_utils.to_categorical(y)\n",
        "model2.fit(X.toarray(), y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXYe6f8my21Y"
      },
      "source": [
        "model2.evaluate(X_test.toarray(), y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptf40YDjh71c",
        "tags": []
      },
      "source": [
        "train.loc[train.Spam == 1].describe() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMMB-9H3IEwm"
      },
      "source": [
        "# Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "nXyY5EUkIGAS",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "source": [
        "# memory limit may be exceeded. Try deleting some objects before running this next section\n",
        "# or copy this section to a different notebook.\n",
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OtFF5X_IZJ4"
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "import gensim.downloader as api\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "6hNrv_O2nPgD",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "source": [
        "api.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WZabbSCjbeT"
      },
      "source": [
        "__deprecated__\n",
        "```python\n",
        "model_w2v = api.load\"word2vec-google-news-300\")\n",
        "```\n",
        "__runtime error__\n",
        "```shell\n",
        "[=========-----------------------------------------] 18.0% 299.5/1662.8MB downloaded\n",
        "---------------------------------------------------------------------------\n",
        "ConnectionResetError                      Traceback (most recent call last)\n",
        "<ipython-input-135-1bce0bfee7a7> in <module>()\n",
        "----> 1 model_w2v = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "7 frames\n",
        "/usr/lib/python3.7/ssl.py in read(self, len, buffer)\n",
        "    927         try:\n",
        "    928             if buffer is not None:\n",
        "--> 929                 return self._sslobj.read(len, buffer)\n",
        "    930             else:\n",
        "    931                 return self._sslobj.read(len)\n",
        "\n",
        "ConnectionResetError: [Errno 104] Connection reset by peer\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53VpEeANkx0l"
      },
      "source": [
        "* [bhaettasch/gensim_word2vec_demo.py](https://gist.github.com/bhaettasch/d7f4e22e79df3c8b6c20)\n",
        "\n",
        "__growupboron__ comment on Aug 24, 2020\n",
        "Since this example this deprecated, I created a [Google Colab demo](https://colab.research.google.com/drive/1aLNhDu1qtQnNIvN5Hhl0UhNBBe84N9S4?usp=sharing) for the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OSTOe4ikPdv"
      },
      "source": [
        "# download and extract the Google News Dataset\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://drive.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://drive.google.com/uc?export=download&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\" -O GoogleNews-vectors-negative300.bin.gz && rm -rf /tmp/cookies.txt\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h9O7A9olWzB"
      },
      "source": [
        "import gensim\n",
        "\n",
        "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
        "#model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, norm_only=True) -> Deprecated\n",
        "model_w2v = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) #without *norm_only* param\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0lxhBMeIYrl",
        "tags": []
      },
      "source": [
        "model_w2v.most_similar(\"cookies\",topn=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rkfcI1niIYQ4",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "source": [
        "model_w2v.doesnt_match([\"USA\",\"Canada\",\"India\",\"Tokyo\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iZ0lCjnaImWZ",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "source": [
        "king = model_w2v['king']\n",
        "man = model_w2v['man']\n",
        "woman = model_w2v['woman']\n",
        "\n",
        "queen = king - man + woman  \n",
        "model_w2v.similar_by_vector(queen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2SSPqujo4K8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}