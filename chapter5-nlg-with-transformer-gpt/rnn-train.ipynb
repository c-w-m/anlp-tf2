{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "anlp37",
      "language": "python",
      "name": "anlp37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "rnn-train.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-w-m/anlp-tf2/blob/master/chapter5-nlg-with-transformer-gpt/rnn-train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgYFUi3f-ENC"
      },
      "source": [
        "!mkdir char-rnn"
      ],
      "id": "EgYFUi3f-ENC",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tCS0uFTCFjU",
        "outputId": "609653bd-d687-40da-efdf-2f9913df8fb6"
      },
      "source": [
        "cd char-rnn"
      ],
      "id": "9tCS0uFTCFjU",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/char-rnn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "pcriiXLcB6Oy",
        "outputId": "3ec3436d-31e1-464b-fa21-5ca1a6312142"
      },
      "source": [
        "# select the 'news-headlines.tsv' file from you local drive to upload to colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "id": "pcriiXLcB6Oy",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-40bee534-ac41-4527-ad7d-a1deb89777af\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-40bee534-ac41-4527-ad7d-a1deb89777af\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving news-headlines.tsv to news-headlines.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDNKW3SHEfgJ",
        "outputId": "5e775410-0b35-4e44-e447-5c34fcfe228c"
      },
      "source": [
        "cd .."
      ],
      "id": "WDNKW3SHEfgJ",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6nTkSG1Ejew",
        "outputId": "11ce3d47-d948-4703-dc50-ae7c7794f7ad"
      },
      "source": [
        "!ls"
      ],
      "id": "J6nTkSG1Ejew",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "char-rnn  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bpcza7IU9eYx"
      },
      "source": [
        "# New Section"
      ],
      "id": "Bpcza7IU9eYx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crude-western"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "import datetime\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "crude-western",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joint-preparation",
        "outputId": "dd81416a-ad71-48e2-c837-390b532c3cdc"
      },
      "source": [
        "######## GPU CONFIGS FOR RTX 2070 ###############\n",
        "## Please ignore if not training on GPU       ##\n",
        "## this is important for running CuDNN on GPU ##\n",
        "\n",
        "tf.keras.backend.clear_session() #- for easy reset of notebook state\n",
        "\n",
        "# chck if GPU can be seen by TF\n",
        "tf.config.list_physical_devices('GPU')\n",
        "#tf.debugging.set_log_device_placement(True)\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Restrict TensorFlow to only use the first GPU\n",
        "  try:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
        "  except RuntimeError as e:\n",
        "    # Visible devices must be set before GPUs have been initialized\n",
        "    print(e)\n",
        "###############################################"
      ],
      "id": "joint-preparation",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 Physical GPUs, 1 Logical GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "specific-insulin"
      },
      "source": [
        "#########################################\n",
        "## DATA NORMALIZATION AND TOKENIZATION ##\n",
        "#########################################\n",
        "\n",
        "chars = sorted(set(\"abcdefghijklmnopqrstuvwxyz0123456789 -,;.!?:’’’/\\|_@#$%ˆ&*˜‘+-=()[]{}' ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\n",
        "chars = list(chars)\n",
        "EOS = '<EOS>'\n",
        "UNK = \"<UNK>\"\n",
        "PAD = \"<PAD>\"  # need to move mask to '0'index for Embedding layer\n",
        "chars.append(UNK)\n",
        "chars.append(EOS)  #end of sentence\n",
        "\n",
        "chars.insert(0, PAD)  # now padding should get index of 0\n",
        "\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(chars)}\n",
        "idx2char = np.array(chars)\n",
        "\n",
        "def char_idx(c):\n",
        "    # takes a character and returns an index\n",
        "    # if character is not in list, returns the unknown token\n",
        "    if c in chars:\n",
        "        return char2idx[c]\n",
        "    \n",
        "    return char2idx[UNK]\n",
        "\n",
        "\n",
        "data = []  # load into this list of lists \n",
        "MAX_LEN = 75  #maximum length of a headline "
      ],
      "id": "specific-insulin",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS-7p19OBJDP",
        "outputId": "943187c5-0328-4d39-b8ae-396f031d4d73"
      },
      "source": [
        "#/content/char-rnn/news-headlines.tsv\n",
        "#./char-rnn/news-headlines.tsv\n",
        "with open(\"/content/char-rnn/news-headlines.tsv\", \"r\") as file:\n",
        "    lines = csv.reader(file, delimiter='\\t')\n",
        "    for line in lines:\n",
        "        hdln = line[0]\n",
        "        cnvrtd = [char_idx(c) for c in hdln[:-1]]  # convert to number\n",
        "        if len(cnvrtd) >= MAX_LEN:\n",
        "            cnvrtd = cnvrtd[0:MAX_LEN-1]\n",
        "            cnvrtd.append(char2idx[EOS])\n",
        "        else:\n",
        "            cnvrtd.append(char2idx[EOS])\n",
        "            # add padding tokens\n",
        "            remain = MAX_LEN - len(cnvrtd)\n",
        "            if remain > 0:\n",
        "                for i in range(remain):\n",
        "                    cnvrtd.append(char2idx[PAD])\n",
        "        data.append(cnvrtd)\n",
        "print(\"**** Data file loaded ****\")"
      ],
      "id": "NS-7p19OBJDP",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**** Data file loaded ****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGHLuBSlBLpv"
      },
      "source": [
        "# now convert to numpy array\n",
        "np_data = np.array(data)\n",
        "\n",
        "# for training, we use one character shifted data\n",
        "np_data_in = np_data[:, :-1]\n",
        "np_data_out = np_data[:, 1:]\n",
        "\n",
        "# Create TF dataset\n",
        "x = tf.data.Dataset.from_tensor_slices((np_data_in, np_data_out))"
      ],
      "id": "WGHLuBSlBLpv",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "continuing-hospital",
        "outputId": "9bae1e69-3a51-40c9-fe26-1f0e93f07344"
      },
      "source": [
        "##################\n",
        "#### MODELING ####\n",
        "##################\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "#batch size\n",
        "BATCH_SIZE=256\n",
        "\n",
        "# create tf.DataSet\n",
        "x_train = x.shuffle(100000, reshuffle_each_iteration=True\n",
        "                   ).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# define the model\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(\n",
        "                  vocab_size = vocab_size,\n",
        "                  embedding_dim=embedding_dim,\n",
        "                  rnn_units=rnn_units,\n",
        "                  batch_size=BATCH_SIZE)\n",
        "\n",
        "print(\"**** Model Instantiated ****\")\n",
        "print(model.summary())\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer = 'adam', loss = loss)\n",
        "\n",
        "# Custom Callback for Learning Rate Decay\n",
        "class LearningRateScheduler(tf.keras.callbacks.Callback):\n",
        "  \"\"\"Learning rate scheduler which decays the learning rate\"\"\"\n",
        "\n",
        "  def __init__(self, init_lr, decay, steps, start_epoch):\n",
        "    super().__init__()\n",
        "    self.init_lr = init_lr  #initial learning rate\n",
        "    self.decay = decay  # how sharply to decay\n",
        "    self.steps = steps  # total number of steps of decay\n",
        "    self.start_epoch = start_epoch  # which epoch to start decaying\n",
        "\n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "    if not hasattr(self.model.optimizer, 'lr'):\n",
        "      raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
        "    # Get the current learning rate from model's optimizer.\n",
        "    lr = float(tf.keras.backend.get_value(\n",
        "                        self.model.optimizer.lr))\n",
        "    if(epoch >= self.start_epoch):\n",
        "        # Call schedule function to get the scheduled learning rate.\n",
        "        scheduled_lr = self.init_lr / (1 + self.decay * (epoch / self.steps))\n",
        "        # Set the value back to the optimizer before this epoch starts\n",
        "        tf.keras.backend.set_value(self.model.optimizer.lr, \n",
        "                                   scheduled_lr)\n",
        "        print('\\nEpoch %05d: Learning rate is %6.8f.' % (epoch, \n",
        "                                                         scheduled_lr))\n",
        "\n",
        "\n",
        "# Setup checkpoints \n",
        "#dynamically build folder names\n",
        "dt = datetime.datetime.today().strftime(\"%Y-%b-%d-%H-%M-%S\")\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints/'+ dt\n",
        "\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "print(\"**** Start Training ****\")\n",
        "EPOCHS=150\n",
        "lr_decay = LearningRateScheduler(0.001, 4., EPOCHS, 10)\n",
        "start = time.time()\n",
        "history = model.fit(x_train, epochs=EPOCHS, \n",
        "                    callbacks=[checkpoint_callback, lr_decay])\n",
        "print(\"**** End Training ****\")\n",
        "print(\"Training time: \", time.time()- start)\n",
        "\n",
        "# Plot accuracies\n",
        "lossplot = \"loss-\" + dt + \".png\"\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.savefig(lossplot)\n",
        "\n",
        "print(\"Saved loss to: \", lossplot)"
      ],
      "id": "continuing-hospital",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**** Model Instantiated ****\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (256, None, 256)          24576     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (256, None, 1024)         3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (256, None, 96)           98400     \n",
            "=================================================================\n",
            "Total params: 4,061,280\n",
            "Trainable params: 4,061,280\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "**** Start Training ****\n",
            "Epoch 1/150\n",
            "2434/2434 [==============================] - 138s 49ms/step - loss: 1.5964\n",
            "Epoch 2/150\n",
            "2434/2434 [==============================] - 117s 48ms/step - loss: 1.0058\n",
            "Epoch 3/150\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.9533\n",
            "Epoch 4/150\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.9327\n",
            "Epoch 5/150\n",
            "2434/2434 [==============================] - 117s 48ms/step - loss: 0.9214\n",
            "Epoch 6/150\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.9149\n",
            "Epoch 7/150\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.9108\n",
            "Epoch 8/150\n",
            "2434/2434 [==============================] - 118s 48ms/step - loss: 0.9089\n",
            "Epoch 9/150\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.9076\n",
            "Epoch 10/150\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.9066\n",
            "Epoch 11/150\n",
            "\n",
            "Epoch 00010: Learning rate is 0.00078947.\n",
            "2434/2434 [==============================] - 118s 49ms/step - loss: 0.8972\n",
            "Epoch 12/150\n",
            "\n",
            "Epoch 00011: Learning rate is 0.00077320.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.8886\n",
            "Epoch 13/150\n",
            "\n",
            "Epoch 00012: Learning rate is 0.00075758.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.8840\n",
            "Epoch 14/150\n",
            "\n",
            "Epoch 00013: Learning rate is 0.00074257.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.8803\n",
            "Epoch 15/150\n",
            "\n",
            "Epoch 00014: Learning rate is 0.00072816.\n",
            "2434/2434 [==============================] - 122s 50ms/step - loss: 0.8770\n",
            "Epoch 16/150\n",
            "\n",
            "Epoch 00015: Learning rate is 0.00071429.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.8745\n",
            "Epoch 17/150\n",
            "\n",
            "Epoch 00016: Learning rate is 0.00070093.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.8715\n",
            "Epoch 18/150\n",
            "\n",
            "Epoch 00017: Learning rate is 0.00068807.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.8694\n",
            "Epoch 19/150\n",
            "\n",
            "Epoch 00018: Learning rate is 0.00067568.\n",
            "2434/2434 [==============================] - 118s 48ms/step - loss: 0.8671\n",
            "Epoch 20/150\n",
            "\n",
            "Epoch 00019: Learning rate is 0.00066372.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.8646\n",
            "Epoch 21/150\n",
            "\n",
            "Epoch 00020: Learning rate is 0.00065217.\n",
            "2434/2434 [==============================] - 122s 50ms/step - loss: 0.8626\n",
            "Epoch 22/150\n",
            "\n",
            "Epoch 00021: Learning rate is 0.00064103.\n",
            "2434/2434 [==============================] - 118s 49ms/step - loss: 0.8604\n",
            "Epoch 23/150\n",
            "\n",
            "Epoch 00022: Learning rate is 0.00063025.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.8580\n",
            "Epoch 24/150\n",
            "\n",
            "Epoch 00023: Learning rate is 0.00061983.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.8560\n",
            "Epoch 25/150\n",
            "\n",
            "Epoch 00024: Learning rate is 0.00060976.\n",
            "2434/2434 [==============================] - 118s 48ms/step - loss: 0.8538\n",
            "Epoch 26/150\n",
            "\n",
            "Epoch 00025: Learning rate is 0.00060000.\n",
            "2434/2434 [==============================] - 122s 50ms/step - loss: 0.8523\n",
            "Epoch 27/150\n",
            "\n",
            "Epoch 00026: Learning rate is 0.00059055.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.8502\n",
            "Epoch 28/150\n",
            "\n",
            "Epoch 00027: Learning rate is 0.00058140.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.8486\n",
            "Epoch 29/150\n",
            "\n",
            "Epoch 00028: Learning rate is 0.00057252.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.8465\n",
            "Epoch 30/150\n",
            "\n",
            "Epoch 00029: Learning rate is 0.00056391.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.8446\n",
            "Epoch 31/150\n",
            "\n",
            "Epoch 00030: Learning rate is 0.00055556.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.8430\n",
            "Epoch 32/150\n",
            "\n",
            "Epoch 00031: Learning rate is 0.00054745.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.8413\n",
            "Epoch 33/150\n",
            "\n",
            "Epoch 00032: Learning rate is 0.00053957.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.8393\n",
            "Epoch 34/150\n",
            "\n",
            "Epoch 00033: Learning rate is 0.00053191.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.8383\n",
            "Epoch 35/150\n",
            "\n",
            "Epoch 00034: Learning rate is 0.00052448.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.8359\n",
            "Epoch 36/150\n",
            "\n",
            "Epoch 00035: Learning rate is 0.00051724.\n",
            "2434/2434 [==============================] - 118s 48ms/step - loss: 0.8348\n",
            "Epoch 37/150\n",
            "\n",
            "Epoch 00036: Learning rate is 0.00051020.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.8332\n",
            "Epoch 38/150\n",
            "\n",
            "Epoch 00037: Learning rate is 0.00050336.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.8308\n",
            "Epoch 39/150\n",
            "\n",
            "Epoch 00038: Learning rate is 0.00049669.\n",
            "2434/2434 [==============================] - 118s 48ms/step - loss: 0.8298\n",
            "Epoch 40/150\n",
            "\n",
            "Epoch 00039: Learning rate is 0.00049020.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.8291\n",
            "Epoch 41/150\n",
            "\n",
            "Epoch 00040: Learning rate is 0.00048387.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.8272\n",
            "Epoch 42/150\n",
            "\n",
            "Epoch 00041: Learning rate is 0.00047771.\n",
            "2434/2434 [==============================] - 118s 48ms/step - loss: 0.8251\n",
            "Epoch 43/150\n",
            "\n",
            "Epoch 00042: Learning rate is 0.00047170.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.8239\n",
            "Epoch 44/150\n",
            "\n",
            "Epoch 00043: Learning rate is 0.00046584.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.8230\n",
            "Epoch 45/150\n",
            "\n",
            "Epoch 00044: Learning rate is 0.00046012.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.8214\n",
            "Epoch 46/150\n",
            "\n",
            "Epoch 00045: Learning rate is 0.00045455.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.8200\n",
            "Epoch 47/150\n",
            "\n",
            "Epoch 00046: Learning rate is 0.00044910.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.8189\n",
            "Epoch 48/150\n",
            "\n",
            "Epoch 00047: Learning rate is 0.00044379.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.8168\n",
            "Epoch 49/150\n",
            "\n",
            "Epoch 00048: Learning rate is 0.00043860.\n",
            "2434/2434 [==============================] - 122s 50ms/step - loss: 0.8160\n",
            "Epoch 50/150\n",
            "\n",
            "Epoch 00049: Learning rate is 0.00043353.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.8144\n",
            "Epoch 51/150\n",
            "\n",
            "Epoch 00050: Learning rate is 0.00042857.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.8146\n",
            "Epoch 52/150\n",
            "\n",
            "Epoch 00051: Learning rate is 0.00042373.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.8118\n",
            "Epoch 53/150\n",
            "\n",
            "Epoch 00052: Learning rate is 0.00041899.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.8115\n",
            "Epoch 54/150\n",
            "\n",
            "Epoch 00053: Learning rate is 0.00041436.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.8105\n",
            "Epoch 55/150\n",
            "\n",
            "Epoch 00054: Learning rate is 0.00040984.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.8085\n",
            "Epoch 56/150\n",
            "\n",
            "Epoch 00055: Learning rate is 0.00040541.\n",
            "2434/2434 [==============================] - 117s 48ms/step - loss: 0.8074\n",
            "Epoch 57/150\n",
            "\n",
            "Epoch 00056: Learning rate is 0.00040107.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.8069\n",
            "Epoch 58/150\n",
            "\n",
            "Epoch 00057: Learning rate is 0.00039683.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.8049\n",
            "Epoch 59/150\n",
            "\n",
            "Epoch 00058: Learning rate is 0.00039267.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.8044\n",
            "Epoch 60/150\n",
            "\n",
            "Epoch 00059: Learning rate is 0.00038860.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.8036\n",
            "Epoch 61/150\n",
            "\n",
            "Epoch 00060: Learning rate is 0.00038462.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.8027\n",
            "Epoch 62/150\n",
            "\n",
            "Epoch 00061: Learning rate is 0.00038071.\n",
            "2434/2434 [==============================] - 118s 48ms/step - loss: 0.8010\n",
            "Epoch 63/150\n",
            "\n",
            "Epoch 00062: Learning rate is 0.00037688.\n",
            "2434/2434 [==============================] - 118s 48ms/step - loss: 0.8007\n",
            "Epoch 64/150\n",
            "\n",
            "Epoch 00063: Learning rate is 0.00037313.\n",
            "2434/2434 [==============================] - 117s 48ms/step - loss: 0.7995\n",
            "Epoch 65/150\n",
            "\n",
            "Epoch 00064: Learning rate is 0.00036946.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7988\n",
            "Epoch 66/150\n",
            "\n",
            "Epoch 00065: Learning rate is 0.00036585.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7974\n",
            "Epoch 67/150\n",
            "\n",
            "Epoch 00066: Learning rate is 0.00036232.\n",
            "2434/2434 [==============================] - 118s 48ms/step - loss: 0.7968\n",
            "Epoch 68/150\n",
            "\n",
            "Epoch 00067: Learning rate is 0.00035885.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7954\n",
            "Epoch 69/150\n",
            "\n",
            "Epoch 00068: Learning rate is 0.00035545.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7948\n",
            "Epoch 70/150\n",
            "\n",
            "Epoch 00069: Learning rate is 0.00035211.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7937\n",
            "Epoch 71/150\n",
            "\n",
            "Epoch 00070: Learning rate is 0.00034884.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7931\n",
            "Epoch 72/150\n",
            "\n",
            "Epoch 00071: Learning rate is 0.00034562.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7922\n",
            "Epoch 73/150\n",
            "\n",
            "Epoch 00072: Learning rate is 0.00034247.\n",
            "2434/2434 [==============================] - 118s 49ms/step - loss: 0.7914\n",
            "Epoch 74/150\n",
            "\n",
            "Epoch 00073: Learning rate is 0.00033937.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7904\n",
            "Epoch 75/150\n",
            "\n",
            "Epoch 00074: Learning rate is 0.00033632.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7896\n",
            "Epoch 76/150\n",
            "\n",
            "Epoch 00075: Learning rate is 0.00033333.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7894\n",
            "Epoch 77/150\n",
            "\n",
            "Epoch 00076: Learning rate is 0.00033040.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7878\n",
            "Epoch 78/150\n",
            "\n",
            "Epoch 00077: Learning rate is 0.00032751.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7871\n",
            "Epoch 79/150\n",
            "\n",
            "Epoch 00078: Learning rate is 0.00032468.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7865\n",
            "Epoch 80/150\n",
            "\n",
            "Epoch 00079: Learning rate is 0.00032189.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7859\n",
            "Epoch 81/150\n",
            "\n",
            "Epoch 00080: Learning rate is 0.00031915.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7851\n",
            "Epoch 82/150\n",
            "\n",
            "Epoch 00081: Learning rate is 0.00031646.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7847\n",
            "Epoch 83/150\n",
            "\n",
            "Epoch 00082: Learning rate is 0.00031381.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7837\n",
            "Epoch 84/150\n",
            "\n",
            "Epoch 00083: Learning rate is 0.00031120.\n",
            "2434/2434 [==============================] - 118s 48ms/step - loss: 0.7831\n",
            "Epoch 85/150\n",
            "\n",
            "Epoch 00084: Learning rate is 0.00030864.\n",
            "2434/2434 [==============================] - 122s 50ms/step - loss: 0.7822\n",
            "Epoch 86/150\n",
            "\n",
            "Epoch 00085: Learning rate is 0.00030612.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7819\n",
            "Epoch 87/150\n",
            "\n",
            "Epoch 00086: Learning rate is 0.00030364.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7814\n",
            "Epoch 88/150\n",
            "\n",
            "Epoch 00087: Learning rate is 0.00030120.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7803\n",
            "Epoch 89/150\n",
            "\n",
            "Epoch 00088: Learning rate is 0.00029880.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7798\n",
            "Epoch 90/150\n",
            "\n",
            "Epoch 00089: Learning rate is 0.00029644.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7794\n",
            "Epoch 91/150\n",
            "\n",
            "Epoch 00090: Learning rate is 0.00029412.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7794\n",
            "Epoch 92/150\n",
            "\n",
            "Epoch 00091: Learning rate is 0.00029183.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7780\n",
            "Epoch 93/150\n",
            "\n",
            "Epoch 00092: Learning rate is 0.00028958.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7775\n",
            "Epoch 94/150\n",
            "\n",
            "Epoch 00093: Learning rate is 0.00028736.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7771\n",
            "Epoch 95/150\n",
            "\n",
            "Epoch 00094: Learning rate is 0.00028517.\n",
            "2434/2434 [==============================] - 118s 49ms/step - loss: 0.7756\n",
            "Epoch 96/150\n",
            "\n",
            "Epoch 00095: Learning rate is 0.00028302.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7753\n",
            "Epoch 97/150\n",
            "\n",
            "Epoch 00096: Learning rate is 0.00028090.\n",
            "2434/2434 [==============================] - 121s 49ms/step - loss: 0.7750\n",
            "Epoch 98/150\n",
            "\n",
            "Epoch 00097: Learning rate is 0.00027881.\n",
            "2434/2434 [==============================] - 118s 49ms/step - loss: 0.7742\n",
            "Epoch 99/150\n",
            "\n",
            "Epoch 00098: Learning rate is 0.00027675.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7736\n",
            "Epoch 100/150\n",
            "\n",
            "Epoch 00099: Learning rate is 0.00027473.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7734\n",
            "Epoch 101/150\n",
            "\n",
            "Epoch 00100: Learning rate is 0.00027273.\n",
            "2434/2434 [==============================] - 117s 48ms/step - loss: 0.7729\n",
            "Epoch 102/150\n",
            "\n",
            "Epoch 00101: Learning rate is 0.00027076.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7722\n",
            "Epoch 103/150\n",
            "\n",
            "Epoch 00102: Learning rate is 0.00026882.\n",
            "2434/2434 [==============================] - 116s 48ms/step - loss: 0.7719\n",
            "Epoch 104/150\n",
            "\n",
            "Epoch 00103: Learning rate is 0.00026690.\n",
            "2434/2434 [==============================] - 122s 50ms/step - loss: 0.7719\n",
            "Epoch 105/150\n",
            "\n",
            "Epoch 00104: Learning rate is 0.00026502.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7711\n",
            "Epoch 106/150\n",
            "\n",
            "Epoch 00105: Learning rate is 0.00026316.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7704\n",
            "Epoch 107/150\n",
            "\n",
            "Epoch 00106: Learning rate is 0.00026132.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7695\n",
            "Epoch 108/150\n",
            "\n",
            "Epoch 00107: Learning rate is 0.00025952.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7694\n",
            "Epoch 109/150\n",
            "\n",
            "Epoch 00108: Learning rate is 0.00025773.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7690\n",
            "Epoch 110/150\n",
            "\n",
            "Epoch 00109: Learning rate is 0.00025597.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7686\n",
            "Epoch 111/150\n",
            "\n",
            "Epoch 00110: Learning rate is 0.00025424.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7683\n",
            "Epoch 112/150\n",
            "\n",
            "Epoch 00111: Learning rate is 0.00025253.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7674\n",
            "Epoch 113/150\n",
            "\n",
            "Epoch 00112: Learning rate is 0.00025084.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7674\n",
            "Epoch 114/150\n",
            "\n",
            "Epoch 00113: Learning rate is 0.00024917.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7663\n",
            "Epoch 115/150\n",
            "\n",
            "Epoch 00114: Learning rate is 0.00024752.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7659\n",
            "Epoch 116/150\n",
            "\n",
            "Epoch 00115: Learning rate is 0.00024590.\n",
            "2434/2434 [==============================] - 122s 50ms/step - loss: 0.7654\n",
            "Epoch 117/150\n",
            "\n",
            "Epoch 00116: Learning rate is 0.00024430.\n",
            "2434/2434 [==============================] - 117s 48ms/step - loss: 0.7650\n",
            "Epoch 118/150\n",
            "\n",
            "Epoch 00117: Learning rate is 0.00024272.\n",
            "2434/2434 [==============================] - 122s 50ms/step - loss: 0.7648\n",
            "Epoch 119/150\n",
            "\n",
            "Epoch 00118: Learning rate is 0.00024116.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7652\n",
            "Epoch 120/150\n",
            "\n",
            "Epoch 00119: Learning rate is 0.00023962.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7647\n",
            "Epoch 121/150\n",
            "\n",
            "Epoch 00120: Learning rate is 0.00023810.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7636\n",
            "Epoch 122/150\n",
            "\n",
            "Epoch 00121: Learning rate is 0.00023659.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7633\n",
            "Epoch 123/150\n",
            "\n",
            "Epoch 00122: Learning rate is 0.00023511.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7635\n",
            "Epoch 124/150\n",
            "\n",
            "Epoch 00123: Learning rate is 0.00023364.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7623\n",
            "Epoch 125/150\n",
            "\n",
            "Epoch 00124: Learning rate is 0.00023220.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7625\n",
            "Epoch 126/150\n",
            "\n",
            "Epoch 00125: Learning rate is 0.00023077.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7621\n",
            "Epoch 127/150\n",
            "\n",
            "Epoch 00126: Learning rate is 0.00022936.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7617\n",
            "Epoch 128/150\n",
            "\n",
            "Epoch 00127: Learning rate is 0.00022796.\n",
            "2434/2434 [==============================] - 117s 48ms/step - loss: 0.7608\n",
            "Epoch 129/150\n",
            "\n",
            "Epoch 00128: Learning rate is 0.00022659.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7612\n",
            "Epoch 130/150\n",
            "\n",
            "Epoch 00129: Learning rate is 0.00022523.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7606\n",
            "Epoch 131/150\n",
            "\n",
            "Epoch 00130: Learning rate is 0.00022388.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7600\n",
            "Epoch 132/150\n",
            "\n",
            "Epoch 00131: Learning rate is 0.00022255.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7598\n",
            "Epoch 133/150\n",
            "\n",
            "Epoch 00132: Learning rate is 0.00022124.\n",
            "2434/2434 [==============================] - 122s 50ms/step - loss: 0.7596\n",
            "Epoch 134/150\n",
            "\n",
            "Epoch 00133: Learning rate is 0.00021994.\n",
            "2434/2434 [==============================] - 118s 49ms/step - loss: 0.7591\n",
            "Epoch 135/150\n",
            "\n",
            "Epoch 00134: Learning rate is 0.00021866.\n",
            "2434/2434 [==============================] - 122s 50ms/step - loss: 0.7585\n",
            "Epoch 136/150\n",
            "\n",
            "Epoch 00135: Learning rate is 0.00021739.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7587\n",
            "Epoch 137/150\n",
            "\n",
            "Epoch 00136: Learning rate is 0.00021614.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7582\n",
            "Epoch 138/150\n",
            "\n",
            "Epoch 00137: Learning rate is 0.00021490.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7575\n",
            "Epoch 139/150\n",
            "\n",
            "Epoch 00138: Learning rate is 0.00021368.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7568\n",
            "Epoch 140/150\n",
            "\n",
            "Epoch 00139: Learning rate is 0.00021246.\n",
            "2434/2434 [==============================] - 119s 49ms/step - loss: 0.7571\n",
            "Epoch 141/150\n",
            "\n",
            "Epoch 00140: Learning rate is 0.00021127.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7564\n",
            "Epoch 142/150\n",
            "\n",
            "Epoch 00141: Learning rate is 0.00021008.\n",
            "2434/2434 [==============================] - 118s 48ms/step - loss: 0.7560\n",
            "Epoch 143/150\n",
            "\n",
            "Epoch 00142: Learning rate is 0.00020891.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7565\n",
            "Epoch 144/150\n",
            "\n",
            "Epoch 00143: Learning rate is 0.00020776.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7556\n",
            "Epoch 145/150\n",
            "\n",
            "Epoch 00144: Learning rate is 0.00020661.\n",
            "2434/2434 [==============================] - 118s 48ms/step - loss: 0.7556\n",
            "Epoch 146/150\n",
            "\n",
            "Epoch 00145: Learning rate is 0.00020548.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7551\n",
            "Epoch 147/150\n",
            "\n",
            "Epoch 00146: Learning rate is 0.00020436.\n",
            "2434/2434 [==============================] - 120s 49ms/step - loss: 0.7548\n",
            "Epoch 148/150\n",
            "\n",
            "Epoch 00147: Learning rate is 0.00020325.\n",
            "2434/2434 [==============================] - 118s 49ms/step - loss: 0.7543\n",
            "Epoch 149/150\n",
            "\n",
            "Epoch 00148: Learning rate is 0.00020216.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7544\n",
            "Epoch 150/150\n",
            "\n",
            "Epoch 00149: Learning rate is 0.00020107.\n",
            "2434/2434 [==============================] - 121s 50ms/step - loss: 0.7541\n",
            "**** End Training ****\n",
            "Training time:  18033.86998295784\n",
            "Saved loss to:  loss-2021-Apr-06-02-46-54.png\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}