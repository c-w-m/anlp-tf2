{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efficient-testing",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/c-w-m/anlp-tf2/blob/master/chapter5-nlg-with-transformer-gpt/rnn-train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "crude-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "joint-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## GPU CONFIGS FOR RTX 2070 ###############\n",
    "## Please ignore if not training on GPU       ##\n",
    "## this is important for running CuDNN on GPU ##\n",
    "\n",
    "tf.keras.backend.clear_session() #- for easy reset of notebook state\n",
    "\n",
    "# chck if GPU can be seen by TF\n",
    "tf.config.list_physical_devices('GPU')\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "specific-insulin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Data file loaded ****\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "## DATA NORMALIZATION AND TOKENIZATION ##\n",
    "#########################################\n",
    "\n",
    "chars = sorted(set(\"abcdefghijklmnopqrstuvwxyz0123456789 -,;.!?:’’’/\\|_@#$%ˆ&*˜‘+-=()[]{}' ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\n",
    "chars = list(chars)\n",
    "EOS = '<EOS>'\n",
    "UNK = \"<UNK>\"\n",
    "PAD = \"<PAD>\"  # need to move mask to '0'index for Embedding layer\n",
    "chars.append(UNK)\n",
    "chars.append(EOS)  #end of sentence\n",
    "\n",
    "chars.insert(0, PAD)  # now padding should get index of 0\n",
    "\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(chars)}\n",
    "idx2char = np.array(chars)\n",
    "\n",
    "def char_idx(c):\n",
    "    # takes a character and returns an index\n",
    "    # if character is not in list, returns the unknown token\n",
    "    if c in chars:\n",
    "        return char2idx[c]\n",
    "    \n",
    "    return char2idx[UNK]\n",
    "\n",
    "\n",
    "data = []  # load into this list of lists \n",
    "MAX_LEN = 75  #maximum length of a headline \n",
    "\n",
    "with open(\"./char-rnn/news-headlines.tsv\", \"r\") as file:\n",
    "    lines = csv.reader(file, delimiter='\\t')\n",
    "    for line in lines:\n",
    "        hdln = line[0]\n",
    "        cnvrtd = [char_idx(c) for c in hdln[:-1]]  # convert to number\n",
    "        if len(cnvrtd) >= MAX_LEN:\n",
    "            cnvrtd = cnvrtd[0:MAX_LEN-1]\n",
    "            cnvrtd.append(char2idx[EOS])\n",
    "        else:\n",
    "            cnvrtd.append(char2idx[EOS])\n",
    "            # add padding tokens\n",
    "            remain = MAX_LEN - len(cnvrtd)\n",
    "            if remain > 0:\n",
    "                for i in range(remain):\n",
    "                    cnvrtd.append(char2idx[PAD])\n",
    "        data.append(cnvrtd)\n",
    "print(\"**** Data file loaded ****\")\n",
    "\n",
    "# now convert to numpy array\n",
    "np_data = np.array(data)\n",
    "\n",
    "# for training, we use one character shifted data\n",
    "np_data_in = np_data[:, :-1]\n",
    "np_data_out = np_data[:, 1:]\n",
    "\n",
    "# Create TF dataset\n",
    "x = tf.data.Dataset.from_tensor_slices((np_data_in, np_data_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "#### MODELING ####\n",
    "##################\n",
    "\n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "#batch size\n",
    "BATCH_SIZE=256\n",
    "\n",
    "# create tf.DataSet\n",
    "x_train = x.shuffle(100000, reshuffle_each_iteration=True\n",
    "                   ).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# define the model\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "model = build_model(\n",
    "                  vocab_size = vocab_size,\n",
    "                  embedding_dim=embedding_dim,\n",
    "                  rnn_units=rnn_units,\n",
    "                  batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"**** Model Instantiated ****\")\n",
    "print(model.summary())\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer = 'adam', loss = loss)\n",
    "\n",
    "# Custom Callback for Learning Rate Decay\n",
    "class LearningRateScheduler(tf.keras.callbacks.Callback):\n",
    "  \"\"\"Learning rate scheduler which decays the learning rate\"\"\"\n",
    "\n",
    "  def __init__(self, init_lr, decay, steps, start_epoch):\n",
    "    super().__init__()\n",
    "    self.init_lr = init_lr  #initial learning rate\n",
    "    self.decay = decay  # how sharply to decay\n",
    "    self.steps = steps  # total number of steps of decay\n",
    "    self.start_epoch = start_epoch  # which epoch to start decaying\n",
    "\n",
    "  def on_epoch_begin(self, epoch, logs=None):\n",
    "    if not hasattr(self.model.optimizer, 'lr'):\n",
    "      raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "    # Get the current learning rate from model's optimizer.\n",
    "    lr = float(tf.keras.backend.get_value(\n",
    "                        self.model.optimizer.lr))\n",
    "    if(epoch >= self.start_epoch):\n",
    "        # Call schedule function to get the scheduled learning rate.\n",
    "        scheduled_lr = self.init_lr / (1 + self.decay * (epoch / self.steps))\n",
    "        # Set the value back to the optimizer before this epoch starts\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, \n",
    "                                   scheduled_lr)\n",
    "        print('\\nEpoch %05d: Learning rate is %6.8f.' % (epoch, \n",
    "                                                         scheduled_lr))\n",
    "\n",
    "\n",
    "# Setup checkpoints \n",
    "#dynamically build folder names\n",
    "dt = datetime.datetime.today().strftime(\"%Y-%b-%d-%H-%M-%S\")\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints/'+ dt\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "print(\"**** Start Training ****\")\n",
    "EPOCHS=150\n",
    "lr_decay = LearningRateScheduler(0.001, 4., EPOCHS, 10)\n",
    "start = time.time()\n",
    "history = model.fit(x_train, epochs=EPOCHS, \n",
    "                    callbacks=[checkpoint_callback, lr_decay])\n",
    "print(\"**** End Training ****\")\n",
    "print(\"Training time: \", time.time()- start)\n",
    "\n",
    "# Plot accuracies\n",
    "lossplot = \"loss-\" + dt + \".png\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig(lossplot)\n",
    "\n",
    "print(\"Saved loss to: \", lossplot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp37",
   "language": "python",
   "name": "anlp37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
